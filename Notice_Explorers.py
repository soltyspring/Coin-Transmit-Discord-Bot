import os
import json
import re
import time
import requests
import cloudscraper
from bs4 import BeautifulSoup

# ----------------------------
# ì„¤ì •
# ----------------------------
LIST_URL = "https://api.bithumb.com/v1/notices"
HEADERS = {"accept": "application/json", "User-Agent": "Mozilla/5.0"}
OUTPUT_FILE = "airdrop_explorers.json"

scraper = cloudscraper.create_scraper()

# ----------------------------
# JSON ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°
# ----------------------------
def save_airdrops(data):
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def load_airdrops():
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# ----------------------------
# í¬ë¡¤ë§ í•¨ìˆ˜
# ----------------------------
def safe_request(url):
    """ì•ˆì „ ìš”ì²­ (2ì´ˆ ì§€ì—° í¬í•¨)"""
    time.sleep(2)
    return scraper.get(url, headers=HEADERS)

def fetch_recent_notices(size=20):
    """ìµœê·¼ ê³µì§€ sizeê°œ ê°€ì ¸ì˜¤ê¸°"""
    resp = scraper.get(   # ğŸ”¥ requests â†’ scraper
        LIST_URL,
        headers=HEADERS,
        params={"page": 1, "count": size}
    )
    resp.raise_for_status()
    return resp.json()

def fetch_notice_links(url):
    """ì´ë²¤íŠ¸ ê³µì§€ì—ì„œ ê±°ë˜ì§€ì› ì•ˆë‚´ ë§í¬ ì¶”ì¶œ"""
    try:
        resp = safe_request(url)
        resp.raise_for_status()
    except Exception as e:
        print(f"âš ï¸ ë§í¬ ìš”ì²­ ì‹¤íŒ¨ ({url}) â†’ {e}")
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    content_div = soup.select_one("div[class^=NoticeDetailContent_detail-content]")
    if not content_div:
        return []

    links = []
    for a in content_div.find_all("a", href=True):
        text = a.get_text(strip=True)
        if re.search(r"/notice/\d+", a["href"]) and ("ê±°ë˜ì§€ì›" in text or "ê±°ë˜ ì§€ì›" in text):
            full_url = a["href"]
            if not full_url.startswith("http"):
                full_url = "https://feed.bithumb.com" + full_url
            links.append({"text": text, "url": full_url})
    return links

def fetch_coins_and_explorers(url):
    """ê±°ë˜ì§€ì› ì•ˆë‚´ ê³µì§€ì—ì„œ ì½”ì¸ ì‹¬ë³¼ + ë¸”ë¡ ìµìŠ¤í”Œë¡œëŸ¬ ì¶”ì¶œ"""
    resp = safe_request(url)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    # ì œëª©ì—ì„œ ì½”ì¸ ì‹¬ë³¼ ì¶”ì¶œ
    title_tag = soup.select_one("h2, h3, [class^=NoticeDetailHeader_title__]")
    coins = []
    if title_tag:
        coins = re.findall(r"\(([A-Za-z0-9]+)\)", title_tag.get_text())

    # ë¸”ë¡ ìµìŠ¤í”Œë¡œëŸ¬ ì¶”ì¶œ
    explorers = []
    content_div = soup.select_one("div[class^=NoticeDetailContent_detail-content]")
    if content_div:
        for a in content_div.find_all("a", href=True):
            if "ë¸”ë¡ ìµìŠ¤í”Œë¡œëŸ¬" in a.get_text(strip=True):
                explorers.append(a["href"])

    # ìˆœì„œëŒ€ë¡œ ë§¤ì¹­
    matched = []
    for coin, exp in zip(coins, explorers):
        matched.append({
            "chain": detect_chain(exp),
            "coin": coin,
            "contract": exp.split("/")[-1]
        })

    return matched

def detect_chain(url):
    if "etherscan.io" in url: return "ETH"
    if "basescan.org" in url: return "BASE"
    if "bscscan.com" in url: return "BSC"
    if "solscan.io" in url: return "SOL"
    return "UNKNOWN"

# ----------------------------
# ì‹¤í–‰
# ----------------------------
if __name__ == "__main__":
    old_data = load_airdrops()
    new_data = []

    notices = fetch_recent_notices(size=20)

for item in notices:
    title = item.get("title", "")
    url = item.get("pc_url")

    # ì—ì–´ë“œë ì´ë²¤íŠ¸ë§Œ ì¶”ì¶œ
    if "ì—ì–´ë“œë" not in title:
        continue

    print(f"ğŸ“Œ ì´ë²¤íŠ¸ ê³µì§€: {title}")
    print("URL:", url)

    # ì´ë²¤íŠ¸ ê³µì§€ì—ì„œ ê±°ë˜ì§€ì› ì•ˆë‚´ ë§í¬ ì¶”ì¶œ
    support_links = fetch_notice_links(url)
    print("ê±°ë˜ì§€ì› ì•ˆë‚´ ë§í¬:", len(support_links), "ê°œ")

    # âœ… ë§í¬ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë„˜ì–´ê°
    if not support_links:
        print("âš ï¸ ê±°ë˜ì§€ì› ì•ˆë‚´ ë§í¬ ì—†ìŒ â†’ ìŠ¤í‚µ")
        continue

    record = {
        "event_title": title,
        "event_url": url,
        "coins": []
    }

    event_coins = re.findall(r"\(([A-Za-z0-9]+)\)", title)

    for link in support_links:
        matched = fetch_coins_and_explorers(link["url"])
        matched = [c for c in matched if c["coin"] in event_coins]
        record["coins"].extend(matched)

    if record["coins"]:
        new_data.append(record)



    save_airdrops(new_data)
    print(f"\nâœ… {OUTPUT_FILE} ì €ì¥ ì™„ë£Œ ({len(new_data)}ê±´)")
